{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept proof - Final project\n",
    "## Evaluation of IA model prediction of respiratory deseases thourgh breathing sounds processing\n",
    "\n",
    "The idea of this concept proof is to make a little first preprocessing of the .wav files that are going to be use for the project, train with them an existing IA model and then show up some metrics, in order to see that the current project could be done in one year, and have a little idea of is going to be done.\n",
    "\n",
    "In order to make this proof, this tutorial has been followed, which makes a very similar task (IA model training with .wav files processing to predict some specific english keywords): https://www.tensorflow.org/tutorials/audio/simple_audio?hl=es-419"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n",
    "\n",
    "The proof will use the following constants in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Constants\n",
    "\n",
    "###Seed for randomness\n",
    "SEED = 42\n",
    "###Paths\n",
    "DATASET_AUDIO_PATH = 'dataset/audio_and_txt_files'\n",
    "DATASET_LABELS_PATH = 'dataset/patient_diagnosis.csv'\n",
    "###IA Model\n",
    "TRAINING_PERCENTAGE = 0.8\n",
    "VALIDATION_PERCENTAGE = 0.1\n",
    "TESTING_PERCENTAGE = 0.1\n",
    "EPOCHS = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "In order that the current proof works, you should import the following libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import soundfile\n",
    "import pywav\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display\n",
    "\n",
    "from data import transform_audio_filenames, preprocess_dataset, get_waveform_and_label, generate_diagnosis_data, get_spectrogram, get_spectrogram_and_label_id, get_diagnosis_list, get_random_file_of_diagnosis\n",
    "from graphs import plot_spectrogram\n",
    "\n",
    "# Set the seed value for experiment reproducibility.\n",
    "seed = SEED\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of audio files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading .wav files and their corresponding labels (diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_diagnosis_data(DATASET_LABELS_PATH,False)\n",
    "diagnosis_list = get_diagnosis_list()\n",
    "audio_filenames = tf.io.gfile.glob(str(DATASET_AUDIO_PATH) + '/*.wav')\n",
    "transform_audio_filenames(audio_filenames)\n",
    "audio_filenames = tf.io.gfile.glob(str(DATASET_AUDIO_PATH) + '/*.wav')\n",
    "audio_filenames = tf.random.shuffle(audio_filenames)\n",
    "num_samples = len(audio_filenames)\n",
    "print('Number of total examples:', num_samples)\n",
    "print('Example file tensor:', audio_filenames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Diagnosis list :', diagnosis_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the different sets of data for training the IA model (train, validation and test), with the proportions set in the constants part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_num_samples = int(num_samples * TRAINING_PERCENTAGE)\n",
    "validation_num_samples = int(num_samples * VALIDATION_PERCENTAGE)\n",
    "testing_num_samples = int(num_samples * TESTING_PERCENTAGE)\n",
    "\n",
    "train_files = audio_filenames[:training_num_samples]\n",
    "val_files = audio_filenames[training_num_samples: training_num_samples + validation_num_samples]\n",
    "test_files = audio_filenames[-testing_num_samples:]\n",
    "\n",
    "print('Training set size', len(train_files))\n",
    "print('Validation set size', len(val_files))\n",
    "print('Test set size', len(test_files))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a tensorflow dataset for the training, all with tuples that have the form (waveform, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "files_ds = tf.data.Dataset.from_tensor_slices(train_files)\n",
    "\n",
    "waveform_ds = files_ds.map(\n",
    "    map_func=get_waveform_and_label,\n",
    "    num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 3\n",
    "n = rows * cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, 20))\n",
    "\n",
    "\n",
    "for i, (audio, label) in enumerate(waveform_ds.take(n)):\n",
    "  r = i // cols\n",
    "  c = i % cols\n",
    "  ax = axes[r][c]\n",
    "  ax.plot(audio.numpy())\n",
    "  ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
    "  label = label.numpy().decode('utf-8')\n",
    "  ax.set_title(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert waveforms to spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for waveform, label in waveform_ds.take(1):\n",
    "  label = label.numpy().decode('utf-8')\n",
    "  spectrogram = get_spectrogram(waveform)\n",
    "\n",
    "print('Label:', label)\n",
    "print('Waveform shape:', waveform.shape)\n",
    "print('Spectrogram shape:', spectrogram.shape)\n",
    "print('Audio playback')\n",
    "display.display(display.Audio(waveform, rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
    "timescale = np.arange(waveform.shape[0])\n",
    "axes[0].plot(timescale, waveform.numpy())\n",
    "axes[0].set_title('Waveform')\n",
    "axes[0].set_xlim([0, 16000])\n",
    "\n",
    "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
    "axes[1].set_title('Spectrogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_ds = waveform_ds.map(\n",
    "  map_func=get_spectrogram_and_label_id,\n",
    "  num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 3\n",
    "n = rows*cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, 20))\n",
    "\n",
    "for i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):\n",
    "  r = i // cols\n",
    "  c = i % cols\n",
    "  ax = axes[r][c]\n",
    "  plot_spectrogram(spectrogram.numpy(), ax)\n",
    "  ax.set_title(get_diagnosis_list()[label_id.numpy()])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = spectrogram_ds\n",
    "val_ds = preprocess_dataset(val_files)\n",
    "test_ds = preprocess_dataset(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "val_ds = val_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for spectrogram, _ in spectrogram_ds.take(1):\n",
    "  input_shape = spectrogram.shape\n",
    "print('Input shape:', input_shape)\n",
    "num_labels = len(diagnosis_list)\n",
    "\n",
    "# Instantiate the `tf.keras.layers.Normalization` layer.\n",
    "norm_layer = layers.Normalization()\n",
    "# Fit the state of the layer to the spectrograms\n",
    "# with `Normalization.adapt`.\n",
    "norm_layer.adapt(data=spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    # Downsample the input.\n",
    "    layers.Resizing(32, 32),\n",
    "    # Normalize.\n",
    "    norm_layer,\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_labels),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = history.history\n",
    "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio = []\n",
    "test_labels = []\n",
    "\n",
    "for audio, label in test_ds:\n",
    "  test_audio.append(audio.numpy())\n",
    "  test_labels.append(label.numpy())\n",
    "\n",
    "test_audio = np.array(test_audio)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(test_audio), axis=1)\n",
    "y_true = test_labels\n",
    "\n",
    "test_acc = sum(y_pred == y_true) / len(y_true)\n",
    "print(f'Test set accuracy: {test_acc:.0%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mtx,\n",
    "            xticklabels=diagnosis_list,\n",
    "            yticklabels=diagnosis_list,\n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_diagnosis = 'Bronchiolitis'\n",
    "\n",
    "sample_file = get_random_file_of_diagnosis(test_diagnosis,DATASET_AUDIO_PATH)\n",
    "\n",
    "if(sample_file is not None):\n",
    "  \n",
    "    print(f\"The sample file chosen is : {sample_file}\")\n",
    "\n",
    "    sample_ds = preprocess_dataset([str(sample_file)])\n",
    "\n",
    "    for spectrogram, label in sample_ds.batch(1):\n",
    "        prediction = model(spectrogram)\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.bar(diagnosis_list, tf.nn.softmax(prediction[0]))\n",
    "        plt.title(f'Predictions for \"{diagnosis_list[label[0]]}\"')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
